{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc5f071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/shrutisaxena/yoga-pose-image-classification-dataset\n",
      "License(s): copyright-authors\n",
      "^C\n",
      "unzip:  cannot find or open yoga-pose-image-classification-dataset.zip, yoga-pose-image-classification-dataset.zip.zip or yoga-pose-image-classification-dataset.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "!kaggle datasets download -d shrutisaxena/yoga-pose-image-classification-dataset\n",
    "!unzip yoga-pose-image-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb84695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755281160.871183 2259491 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1755281160.934857 2588732 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755281160.943712 2588738 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "#for pose detection\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "#for image input output operations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pimg\n",
    "\n",
    "mp_pose = mp.solutions.pose.Pose(min_detection_confidence=0.7,\n",
    "                                 min_tracking_confidence=0.7)\n",
    "\n",
    "# img = pimg.imread('dataset/upavistha konasana/2-0.png')\n",
    "# if img.dtype != 'uint8':\n",
    "#     img = (img * 255).astype('uint8')\n",
    "\n",
    "# results = mp_pose.process(img)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401c572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "mp_drawing.draw_landmarks(\n",
    "    img,\n",
    "    results.pose_landmarks,\n",
    "    mp.solutions.pose.POSE_CONNECTIONS,\n",
    "    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=4, circle_radius=2),\n",
    "    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=6, circle_radius=2)\n",
    "\n",
    ")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60c93e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5993 files belonging to 107 classes.\n",
      "Using 4196 files for training.\n",
      "Found 5993 files belonging to 107 classes.\n",
      "Using 1797 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data_directory = 'dataset'\n",
    "\n",
    "img_hieght = 224,\n",
    "img_width = 224,\n",
    "img_size = (224, 224)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory, \n",
    "    validation_split = 0.3,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "validation_dataset = validation_dataset.skip(val_batches//2)\n",
    "test_dataset = validation_dataset.take(val_batches//2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce2b8b",
   "metadata": {},
   "source": [
    "Goal: Process the dataset to get keypoints data and store in a CSV for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab56a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m labels_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m nan_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraining_dataset\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     14\u001b[0m         img \u001b[38;5;241m=\u001b[39m images[i]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "landmark_data = []\n",
    "\n",
    "# images, labels = next(iter(training_dataset.take(1)))\n",
    "# result = mp_pose.process(img)\n",
    "# print(result.pose_landmarks.landmark)\n",
    "\n",
    "#Extracting Key Point Data to add to csv file. \n",
    "key_points_list = []\n",
    "labels_list = []\n",
    "nan_count = 0\n",
    "for images, labels in training_dataset:\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        label = labels[i].numpy()\n",
    "        result = mp_pose.process(img)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for lm in result.pose_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            key_points_list.append(keypoints)\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            nan_count +=1\n",
    "            landmarks = np.zeros(33 * 3)\n",
    "            key_points_list.append( landmarks)\n",
    "            labels_list.append(label)\n",
    "\n",
    "df = pd.DataFrame(key_points_list)\n",
    "df['label'] = labels_list\n",
    "df.to_csv(\"training_yoga1_keypoints.csv\", index=False)\n",
    "\n",
    "\n",
    "key_points_list = []\n",
    "labels_list = []\n",
    "for images, labels in validation_dataset:\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        label = labels[i].numpy()\n",
    "        result = mp_pose.process(img)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for lm in result.pose_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            key_points_list.append(keypoints)\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            nan_count +=1\n",
    "            landmarks = np.zeros(33 * 3)\n",
    "            key_points_list.append( landmarks)\n",
    "            labels_list.append(label)\n",
    "\n",
    "df = pd.DataFrame(key_points_list)\n",
    "df['label'] = labels_list\n",
    "df.to_csv(\"validation_yoga1_keypoints.csv\", index=False)\n",
    "\n",
    "key_points_list = []\n",
    "labels_list = []\n",
    "for images, labels in test_dataset:\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        label = labels[i].numpy()\n",
    "        result = mp_pose.process(img)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for lm in result.pose_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            key_points_list.append(keypoints)\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            nan_count +=1\n",
    "            landmarks = np.zeros(33 * 3)\n",
    "            key_points_list.append(landmarks)\n",
    "            labels_list.append(label)\n",
    "print(nan_count)\n",
    "df = pd.DataFrame(key_points_list)\n",
    "df['label'] = labels_list\n",
    "df.to_csv(\"test_yoga1_keypoints.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91273dc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff47730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e81b3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
