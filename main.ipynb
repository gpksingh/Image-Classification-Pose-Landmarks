{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1cc5f071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: kaggle\n",
      "Archive:  yoga-pose-image-classification-dataset.zip\n",
      "replace dataset/adho mukha svanasana/1. 1.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/IPython/utils/_process_posix.py:156\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/spawnbase.py:383\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkaggle datasets download -d shrutisaxena/yoga-pose-image-classification-dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munzip yoga-pose-image-classification-dataset.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:685\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:578\u001b[0m, in \u001b[0;36mspawn.sendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    577\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/Image Classification/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:569\u001b[0m, in \u001b[0;36mspawn.send\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    568\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mencode(s, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "!kaggle datasets download -d shrutisaxena/yoga-pose-image-classification-dataset\n",
    "!unzip yoga-pose-image-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb84695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755281160.871183 2259491 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1755281160.934857 2588732 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755281160.943712 2588738 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "#for pose detection\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "#for image input output operations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pimg\n",
    "\n",
    "mp_pose = mp.solutions.pose.Pose(min_detection_confidence=0.7,\n",
    "                                 min_tracking_confidence=0.7)\n",
    "\n",
    "# img = pimg.imread('dataset/upavistha konasana/2-0.png')\n",
    "# if img.dtype != 'uint8':\n",
    "#     img = (img * 255).astype('uint8')\n",
    "\n",
    "# results = mp_pose.process(img)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401c572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "mp_drawing.draw_landmarks(\n",
    "    img,\n",
    "    results.pose_landmarks,\n",
    "    mp.solutions.pose.POSE_CONNECTIONS,\n",
    "    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=4, circle_radius=2),\n",
    "    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=6, circle_radius=2)\n",
    "\n",
    ")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60c93e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5993 files belonging to 107 classes.\n",
      "Using 4196 files for training.\n",
      "Found 5993 files belonging to 107 classes.\n",
      "Using 1797 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data_directory = 'dataset'\n",
    "\n",
    "img_hieght = 224,\n",
    "img_width = 224,\n",
    "img_size = (224, 224)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory, \n",
    "    validation_split = 0.3,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "validation_dataset = validation_dataset.skip(val_batches//2)\n",
    "test_dataset = validation_dataset.take(val_batches//2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce2b8b",
   "metadata": {},
   "source": [
    "Goal: Process the dataset to get keypoints data and store in a CSV for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ab56a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 17:40:05.657400: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2025-08-17 17:40:31.544403: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: cHRM: inconsistent chromaticities\n",
      "2025-08-17 17:40:50.816118: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2025-08-17 17:40:53.434062: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2025-08-17 17:40:56.034721: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2025-08-17 17:41:08.134650: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "landmark_data = []\n",
    "\n",
    "# images, labels = next(iter(training_dataset.take(1)))\n",
    "# result = mp_pose.process(img)\n",
    "# print(result.pose_landmarks.landmark)\n",
    "\n",
    "#Extracting Key Point Data to add to csv file. \n",
    "key_points_list = []\n",
    "labels_list = []\n",
    "nan_count = 0\n",
    "for images, labels in training_dataset:\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        label = labels[i].numpy()\n",
    "        result = mp_pose.process(img)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for lm in result.pose_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            key_points_list.append(keypoints)\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            nan_count +=1\n",
    "            landmarks = np.zeros(33 * 3)\n",
    "            key_points_list.append( landmarks)\n",
    "            labels_list.append(label)\n",
    "\n",
    "df = pd.DataFrame(key_points_list)\n",
    "df['label'] = labels_list\n",
    "df.to_csv(\"training_yoga_keypoints.csv\", index=False)\n",
    "\n",
    "\n",
    "key_points_list = []\n",
    "labels_list = []\n",
    "for images, labels in validation_dataset:\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        label = labels[i].numpy()\n",
    "        result = mp_pose.process(img)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for lm in result.pose_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            key_points_list.append(keypoints)\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            nan_count +=1\n",
    "            landmarks = np.zeros(33 * 3)\n",
    "            key_points_list.append( landmarks)\n",
    "            labels_list.append(label)\n",
    "\n",
    "df = pd.DataFrame(key_points_list)\n",
    "df['label'] = labels_list\n",
    "df.to_csv(\"validation_yoga_keypoints.csv\", index=False)\n",
    "\n",
    "key_points_list = []\n",
    "labels_list = []\n",
    "for images, labels in test_dataset:\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        label = labels[i].numpy()\n",
    "        result = mp_pose.process(img)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for lm in result.pose_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            key_points_list.append(keypoints)\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            nan_count +=1\n",
    "            landmarks = np.zeros(33 * 3)\n",
    "            key_points_list.append(landmarks)\n",
    "            labels_list.append(label)\n",
    "print(nan_count)\n",
    "df = pd.DataFrame(key_points_list)\n",
    "df['label'] = labels_list\n",
    "df.to_csv(\"test_yoga_keypoints.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91273dc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff47730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e81b3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
